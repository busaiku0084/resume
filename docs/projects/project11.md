<details>
  <summary>
    電力市場価格配信のパフォーマンスチューニング
    <span>2025年/5ヶ月</span>
  </summary>
  <div>
    <ul>
      <li><strong>カテゴリ:</strong> <span>webサービス</span> <span>自社</span></li>
      <li><strong>担当工程:</strong> <span>コーディング</span> <span>テスト</span> <span>運用/保守</span></li>
      <li><strong>職種・役割:</strong> <span>バックエンド</span> <span>インフラ</span></li>
      <li><strong>使用技術:</strong> <span>AWS</span> <span>CI/CD</span> <span>Git</span> <span>Python</span> <span>AWS RedShift</span> <span>AWS Glue</span> <span>Athena</span> <span>Lambda</span> <span>Tableau</span></li>
  </div>
  <div class="markdown-content">

## プロジェクト概要

電力市場価格配信のパフォーマンスチューニング

## チーム情報

チーム人数：4名

## 開発・実装内容

※ 2024年12月(プロジェクト途中)から参画

### 【概要】
電力市場価格データを扱うシステムのデータ処理パフォーマンス最適化を目的として、CSVからParquet形式への移行、GlueとRedshiftの最適化、Redshiftノード変更、Tableauのデータ抽出方法改善を実施。私は データ加工・変換部分とGlueによるRedshiftへのデータ投入を担当。

既存システムは拡張性がゼロに近く、また、データ処理の遅延が発生していたため、システムリファクタリング、開発環境の整備、静的解析の導入、ライフサイクル管理の適用なども提案・実施。これにより、システムの保守性を向上させ、データの一貫性を確保。

### 【内容】

#### 1. データ処理の最適化
- CSV から Apache Parquet への移行（担当）
  - 既存のCSVベースのデータ処理をParquet形式に移行し、データ読み書きのパフォーマンスを大幅に向上。
  - 列圧縮やパーティショニングを適用し、クエリの最適化を実施。
- AWS Glue によるデータ加工・変換（担当）
  - AWS Glue を利用して、加工後のParquetデータをAmazon Redshiftに投入。
  - 従来の処理ロジックがブラックボックス化していたため、リファクタリングを行い、拡張性を確保。
- Redshiftのテーブルチューニングを実施（担当）
  - 分散キー・ソートキーを最適化し、クエリ性能を向上。
  - データスキャン量を削減し、Redshiftクエリのレスポンスタイムを短縮。

- RedshiftのVIEWの変更（別のエンジニア担当）
  - クエリの実行効率を向上させるため、不要なカラムを削除し、参照テーブルを見直し。
  - 一部の処理をマテリアライズドビュー化し、頻繁に使用されるデータの取得を高速化。
- Amazon Redshift のノード変更とクエリ最適化（別のエンジニア担当）
  - 従来の dc2.large から ra3.xlplus へ変更し、パフォーマンスを向上。
  - クエリの最適化とマテリアライズドビューの適用により、データ取得速度を高速化。

#### 2. Tableau 向けデータ配信の最適化（別のエンジニア担当）
- Athena を利用したデータ抽出の最適化
  - これまでRedshiftから直接データを取得していたが、Athenaを活用し、Tableauのパフォーマンスを向上。

#### 3. 開発環境の改善（提案・実施）
- ライフサイクル管理の導入
  - S3のデータ保持ポリシーを見直し、不要なデータを自動削除する仕組みを導入。これによりストレージコストを削減し、運用負荷を軽減。
- 静的解析の導入とリファクタリング
  - コードの可読性・保守性向上のため、Flake8, Black などを導入。
- Git の運用改善
  - 外注されていたコードのGit履歴が乱雑だったため、適切なブランチ戦略を策定し、Git運用ルールを整理。
  - ドキュメントを整備し、チーム全体での作業効率を向上。

### 【課題・問題点】
- CSVからParquetへの移行が想定以上に困難
  - 既存のデータ加工処理がCSV形式に依存しすぎており、変換に多くの修正が必要だった。
  - Glueの処理が複雑化し、リファクタリングしながらの移行となった。
- ドキュメント不足
  - これまでの処理がほぼブラックボックス化しており、仕様の把握が困難だった。
- 開発環境の整備不足
  - 外注していたコードの品質が低く、静的解析やバージョン管理のルールがなかった。

### 【成果】
- データ処理速度（加工から配信まで）が約3倍に向上（Parquet + Glueの最適化）
- ストレージコストを月50万円削減（S3ライフサイクル管理の導入）
- 開発環境の整備により、今後の運用負担を軽減

### 【今後の展望】
- さらなるパフォーマンス向上
  - Redshiftのさらなる最適化、データ圧縮方式の改善を検討。
- 技術負債の解消とリプレイスの検討
  - 依存関係が強い古いコードの刷新と、アーキテクチャの見直しが必要。
- データパイプラインの自動化
  - Glueのジョブスケジューリングを最適化し、運用負荷を削減。

  </div>
</details>
